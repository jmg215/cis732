# Notes for Wednesday

## finishing of clustering (02C of 41)
see lecture videos
## and data mining (hybrid of supervised and unsupervised)

## 03A of 41 - clustering and data minding

[readings-today]:ch2 murphy (probability)
[readings-next]:ch3 Mitchel (decision trees)

clustering as a form of preprocessing and as a stand-alone tool

clustering for data understanding and applications
-cluster: formation, segmentation and labeling
1. formation (dimensionality reduction is a side effect of formation)
2. segmentation - defining the boundary (if using k-means, can use nearest neighbors)
3. labeling

-(from monday) heirarchical, partition
-(from today) density based
remapped instance space

in k-means, generally in 1D (apriori is 1D)
-something about same dimension ( from X to X') 
-something about HAC being reducing
-nearest neighbor in 2D and 3D

-finding of boundaries and whaat they mean
-(lab 2 and hw 2 are logistic regression)

### Clustering as a preprocessing tool (utiilty)
#### summarization
PCA - what the primary components (the comps with the most weight on the results) are
- if the weight drops to zero get rid of feature, if you change the weight. something about Gaussian PCA. 
- rank them and remap based on smaller span 
- number of linearly indepenedent axis is less than the original number
Regression - 
Classification - 
heirarchical - "gives us the tree from which to cut?" / "how do you get K" - $byou tell it a cut height, it gives you K
#### compression
- image procession: vector quantization
- automate from a bitmap of the object, find boundary of object in terms of centroid of the shape, define shapes
- phoneme shapes (take training pockets into descrete vectors of a,e,i,o,u sounds) or something. 
- centroid is to mean as medoid is to median
#### finding k-nearest neighbors

#### outlier detection

### measuring wuality of clustering
#### dissimilarity/similarity metric
similarity is expressed in terms of a distance function (specific function that only work on specific types:)
- something about categorical
- rank ordered
- boolean
- oridinal tratio
#### quality of clustering
- measure te goodness of a cluster

### considerations


## 03B of 41
### BIRCH model
-in memory algorithm (in memory CF tree)
-scales linearly
-limitation is that it only handles numeric data

#### CF tree
-cluster feature vector
-there is a root, non-leaf node, and leaf nodes
-CLUSTER DIAMETER eqn (the BIRCH algorithm) which is a greedy algorithm, a hierarchical type algorithm


## 03C of 41
### desnisty based clustering: Basic concepts
two parameters
-density reachable vs density connected

#### DBSCAN
drop r, make the radius tighter

