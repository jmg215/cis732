# notes for Friday

tf.compat.v1 or something to get the program to work

## finishing up BIRCH (4a of 41)

## overview of reiforcement (4b of 41)

- write algorithms for writing these features
(supervised not so good for object detection)
- want objects to be continuous and therefore a single closed curve (with soem tolerance)
- the bounding box is not too tight around the object of interest (deep learning stack has already down sampled it- pyrimid of sampled bitmaps)
- progressive encoders
- formation is moving objects into unnamed categories (As and Bs)
- segmentation is defining the separating surfaces btwn A and B (possibly with K-nearest neighbors)

## unsupervised learning 
-unimodal or bi-modal
-(3d point cloud to 2D heat map)

## reinforcement
- we have learned classification, and clustering (patterns in data), data mining
- but now we watn self-driving cars, so wtf do we do
- or speach recoginiction (map acoustical signals as a sequence of phonemes)
- sequence to sequence mapping is called transduction
- **** learn how to take actions in order to max reward ****
- intelligent control only has feedback from the car (feedback is the reward)
- branches of machine learning


## reinforcement characteristics
- there is no supervision, only a reward signl (the loss function)
- [loss_function] = murphy ch2 and the paper review
- openAI (cart-pole problem) in its ML learning library
- unity reinforcement system (biped,quadr,walkers...)
- muti-agent competeitive, 

## mathematicallly formalzing the markoc devision process and states of the game
-markov devision process starts at t=0 at initial state s-naught
- tilde (~) means "distributied according to"