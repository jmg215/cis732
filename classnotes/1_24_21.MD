# notes for today

## rewatch of Friday (0C of 41 - 1/21/22)

2^5 * 3 = 96 POSSIBILITIES for the table of water sports (data set)
96 possibilities leads to 2^96 possible LABELINGS


N = 6 (col)
M = 4 (row)

concept learning = taking the 96 possible inputs and map to 
boolean output. learning the concept of enjoy sport = true

lattice is the same size as the POWER SET (2^96)

jump one level for each "dont care"


## Monday

### (1B of 41)

Supervised inductive learning:
concept and classification learning vs regression

Candidiate elimination algorithm:

row = instance (n-tuple)

*classic examples

classificiation vs regression: target output, error measure (loss)
-output layer, cost function
-square the error bc of theta (linear coeff estimator); derivative of the function find local extrema
-absolute error (different differential properties than square
-cross entropy measures uncertainty in class labels given input weights

inductive learning hypothesis
-approximate well (how many samples do i need to get a credible confidence interval)
-algorithmic (preference) bias: greedy algorithm searches for shallow search trees; certain loss fucntion op[timizes for certain constraints

prototypical concept learning tasks
-false positives and false negaitves

### (1C of 41)
Version Spaces, Candidate Elmination, Conjuncive Hypothesis Language
-maintain, through necessary and sufficient conditions, the version space (G and H)
-the S-set [Find-S], the floor, the set of sufficient cond s.t. if you meet all of them you have a pos example
-the floor is initially false, then if pos, the floor is [h1]. every time this happens it raises the floor. 
-not guaranteed to terminate

[find-s]:
init h to the most specific hypothesis in H
-partially ordered set under relation Less-Specific-Than
-ability to calculate a least-upper-bound by raising the floor
: conjunctive language ("something to do with AND")


